{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training and Test Sets** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('../dataset/standardized_training.csv')\n",
    "X_train = training_set.drop('general_two_year', axis=1)\n",
    "y_train = training_set['general_two_year']\n",
    "\n",
    "test_set = pd.read_csv('../dataset/standardized_testing.csv')\n",
    "X_test = test_set.drop('general_two_year', axis=1)\n",
    "y_test = test_set['general_two_year'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CART** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5029585798816568\n"
     ]
    }
   ],
   "source": [
    "#basic cart:\n",
    "cart = DecisionTreeClassifier(random_state=42)\n",
    "cart.fit(X_train, y_train)\n",
    "\n",
    "cart_pred = cart.predict(X_test)\n",
    "cart_score = f1_score(y_test, cart_pred)\n",
    "print(cart_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "Best Hyperparameters: {'max_depth': 5, 'max_leaf_nodes': 10, 'min_samples_leaf': 40, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tuning \n",
    "\n",
    "cart_param_grid = {\n",
    "    'max_depth': [1, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [10, 20, 40, 50],\n",
    "    'max_leaf_nodes': [2, 10, 15]\n",
    "}\n",
    "\n",
    "cart_grid_search = GridSearchCV(estimator=cart, param_grid=cart_param_grid, cv=5, n_jobs=-1, verbose=2, scoring='f1')\n",
    "cart_grid_search.fit(X_train, y_train)\n",
    "print(\"Best Hyperparameters:\", cart_grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.5705705705705705\n"
     ]
    }
   ],
   "source": [
    "#best cart:\n",
    "best_cart = DecisionTreeClassifier(**cart_grid_search.best_params_)\n",
    "best_cart.fit(X_train, y_train)\n",
    "best_cart_pred = best_cart.predict(X_test)\n",
    "best_cart_f1 = f1_score(y_test, best_cart_pred)\n",
    "print(\"f1 score:\", best_cart_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EBM** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.5741324921135647\n"
     ]
    }
   ],
   "source": [
    "#basic ebm:\n",
    "ebm = ExplainableBoostingClassifier(random_state=42, n_jobs=-1)\n",
    "ebm.fit(X_train, y_train)\n",
    "\n",
    "ebm_pred = ebm.predict(X_test)\n",
    "ebm_f1 = f1_score(y_test, ebm_pred)\n",
    "print(\"f1 score:\", ebm_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'interactions': 20, 'learning_rate': 0.1, 'min_samples_leaf': 2}\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tuning \n",
    "\n",
    "ebm_param_grid = {\n",
    "    'learning_rate': [0.1],\n",
    "    #'max_bins': [128, 256],\n",
    "    #'max_interaction_bins': [16, 32],\n",
    "    'interactions': [10,20],\n",
    "    'min_samples_leaf': [2,10]\n",
    "}\n",
    "\n",
    "ebm_grid_search = GridSearchCV(estimator=ebm, param_grid=ebm_param_grid, cv=5, n_jobs=-1, verbose=2, scoring='f1')\n",
    "ebm_grid_search.fit(X_train, y_train)\n",
    "print(\"Best Hyperparameters:\", ebm_grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.5932721712538226\n"
     ]
    }
   ],
   "source": [
    "#best ebm:\n",
    "best_ebm = ExplainableBoostingClassifier(**ebm_grid_search.best_params_)\n",
    "best_ebm.fit(X_train, y_train)\n",
    "best_ebm_pred = best_ebm.predict(X_test)\n",
    "best_ebm_f1 = f1_score(y_test, best_ebm_pred)\n",
    "print(\"f1 score:\", best_ebm_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear SVM** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lnick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.5975609756097562\n"
     ]
    }
   ],
   "source": [
    "#basic linear svm:\n",
    "lsvm = LinearSVC(random_state=42, max_iter=100000)\n",
    "lsvm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lsvm.predict(X_test)\n",
    "lsvm_f1 = f1_score(y_test, y_pred)\n",
    "print(\"f1 score:\", lsvm_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'C': 0.1, 'intercept_scaling': 0.1, 'loss': 'hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lnick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tuning \n",
    "\n",
    "lsvm_param_grid = {\n",
    "    'C': [0.1, 1.0, 10],\n",
    "    'intercept_scaling': [0.1, 1, 10],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "}\n",
    "\n",
    "\n",
    "lsvm_grid_search = GridSearchCV(estimator=lsvm, param_grid=lsvm_param_grid, cv=5, n_jobs=-1, verbose=2, scoring='f1')\n",
    "lsvm_grid_search.fit(X_train, y_train)\n",
    "print(\"Best Hyperparameters:\", lsvm_grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.6149253731343284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lnick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#best lsvm:\n",
    "best_lsvm = LinearSVC(**lsvm_grid_search.best_params_, max_iter=10000)\n",
    "best_lsvm.fit(X_train, y_train)\n",
    "best_lsvm_pred = best_lsvm.predict(X_test)\n",
    "best_lsvm_f1 = f1_score(y_test, best_lsvm_pred)\n",
    "print(\"f1 score:\", best_lsvm_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGBoost** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.5062500000000001\n"
     ]
    }
   ],
   "source": [
    "#basic xgboost:\n",
    "xgboost = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgboost.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgboost.predict(X_test)\n",
    "xgb_f1 = f1_score(y_test, y_pred)\n",
    "print(\"f1 score:\", xgb_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Best Hyperparameters: {'colsample_bytree': 0.4, 'gamma': 1, 'max_depth': 1, 'min_child_weight': 40, 'n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tuning \n",
    "\n",
    "xgb_param_grid = {\n",
    "    #'n_estimators': [100, 250, 500], \n",
    "    'n_estimators': [40, 50, 60], \n",
    "    'max_depth': [1, 2, 3],      \n",
    "    'min_child_weight': [20, 30, 40],   \n",
    "    'colsample_bytree': [0.4, 0.5, 0.6],\n",
    "    'gamma': [0, 1, 2]       \n",
    "}\n",
    "\n",
    "\n",
    "xgb_grid_search = GridSearchCV(estimator=xgboost, param_grid=xgb_param_grid, cv=5, n_jobs=-1, verbose=2, scoring='f1')\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "print(\"Best Hyperparameters:\", xgb_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "#best xgb:\n",
    "best_xgb = xgb.XGBClassifier(**xgb_grid_search.best_params_)\n",
    "best_xgb.fit(X_train, y_train)\n",
    "best_xgb_pred = best_xgb.predict(X_test)\n",
    "best_xgb_f1 = f1_score(y_test, best_xgb_pred)\n",
    "print(\"f1 score:\", best_xgb_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Neural Network** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = K.cast(y_true, 'bool')\n",
    "        y_pred = K.cast(K.round(y_pred), 'bool')\n",
    "\n",
    "        # Cast boolean tensors to float\n",
    "        true_positives = K.cast(K.sum(K.cast(y_true & y_pred, 'float32')), 'float32')\n",
    "        false_positives = K.cast(K.sum(K.cast(~y_true & y_pred, 'float32')), 'float32')\n",
    "        false_negatives = K.cast(K.sum(K.cast(y_true & ~y_pred, 'float32')), 'float32')\n",
    "\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.false_positives.assign_add(false_positives)\n",
    "        self.false_negatives.assign_add(false_negatives)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives + K.epsilon())\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + K.epsilon())\n",
    "        f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "        return f1\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 1ms/step - loss: 1.1646 - f1_score: 0.4067\n",
      "f1 score: 0.4066980481147766\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "neural_net = Sequential()\n",
    "neural_net.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "neural_net.add(Dense(64, activation='relu'))\n",
    "neural_net.add(Dense(1, activation='sigmoid')) #for binary features\n",
    "\n",
    "neural_net.compile(optimizer='adam', loss='binary_crossentropy', metrics=[F1Score()])\n",
    "history = neural_net.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "loss, nn_f1 = neural_net.evaluate(X_test, y_test)\n",
    "print(\"f1 score:\", nn_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer, activation):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation=activation, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[F1Score()])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'optimizer': 'adam', 'activation': 'relu', 'batch_size': 32, 'epoch': 10}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tune, X_tunetest, y_tune, y_tunetest = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#hyperparameter grid\n",
    "optimizers = ['adam', 'adagrad', 'sgd']\n",
    "activations = ['relu', 'tanh']\n",
    "batch_sizes = [32, 64]\n",
    "epochs = [10, 50]\n",
    "\n",
    "best_nn_f1 = 0\n",
    "best_nn_params = {}\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    for activation in activations:\n",
    "        for sizes in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                # Create and train the model\n",
    "                nn = create_model(optimizer=optimizer, activation=activation)\n",
    "                nn.fit(X_tune, y_tune, epochs=epoch, batch_size=sizes, verbose=0)\n",
    "                \n",
    "                # Evaluate the model\n",
    "                loss, nn_f1 = nn.evaluate(X_tunetest, y_tunetest, verbose=0)\n",
    "                \n",
    "                # Compare and store the best parameters\n",
    "                if nn_f1 > best_nn_f1:\n",
    "                    best_nn_f1 = nn_f1\n",
    "                    best_nn_params = {'optimizer': optimizer, 'activation': activation, \n",
    "                                      'batch_size': sizes, 'epoch': epoch}\n",
    "\n",
    "print(f\"Best Parameters: {best_nn_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 1ms/step\n",
      "f1 score: 0.5244956772334294\n"
     ]
    }
   ],
   "source": [
    "#best nn:\n",
    "best_nn = create_model(optimizer=best_nn_params['optimizer'], activation=best_nn_params['activation'])\n",
    "best_nn.fit(X_train, y_train, epochs=epoch, batch_size=sizes, verbose=0)\n",
    "\n",
    "best_nn_pred = best_nn.predict(X_test)\n",
    "best_nn_pred = (best_nn_pred > 0.5).astype(int).ravel()\n",
    "best_nn_f1 = f1_score(y_test, best_nn_pred)\n",
    "print(\"f1 score:\", best_nn_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LINEAR SVM WORKED THE BEST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was expected, based on the published study. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
