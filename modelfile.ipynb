{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lnick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training and Test Sets** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('dataset/standardized_training.csv')\n",
    "X_train = training_set.loc[:,:'five_year']\n",
    "y_train = training_set['general_two_year'].values\n",
    "\n",
    "test_set = pd.read_csv('dataset/standardized_testing.csv')\n",
    "X_test = test_set.loc[:,:'five_year']\n",
    "y_test = test_set['general_two_year'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CART** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5805626598465473\n"
     ]
    }
   ],
   "source": [
    "#basic cart:\n",
    "cart = DecisionTreeClassifier(random_state=42)\n",
    "cart.fit(X_smote, y_smote)\n",
    "\n",
    "cart_pred = cart.predict(X_test)\n",
    "cart_accuracy = accuracy_score(y_test, cart_pred)\n",
    "print(\"Accuracy:\", cart_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 5, 'max_leaf_nodes': 15, 'min_samples_leaf': 40, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tuning \n",
    "\n",
    "cart_param_grid = {\n",
    "    'max_depth': [1, 5, 10, 100],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [10, 20, 40, 50, 100],\n",
    "    'max_leaf_nodes': [2, 10, 15, 20]\n",
    "}\n",
    "\n",
    "cart_grid_search = GridSearchCV(estimator=cart, param_grid=cart_param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "cart_grid_search.fit(X_smote, y_smote)\n",
    "print(\"Best Hyperparameters:\", cart_grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Best Hyperparameters: 0.6061381074168798\n"
     ]
    }
   ],
   "source": [
    "#best cart:\n",
    "\n",
    "best_cart = DecisionTreeClassifier(**cart_grid_search.best_params_)\n",
    "best_cart.fit(X_smote, y_smote)\n",
    "best_cart_pred = best_cart.predict(X_test)\n",
    "best_cart_accuracy = accuracy_score(y_test, best_cart_pred)\n",
    "print(\"Accuracy with Best Hyperparameters:\", best_cart_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6367713004484304\n"
     ]
    }
   ],
   "source": [
    "best_cart_train = best_cart.predict(X_train)\n",
    "print(accuracy_score(y_train, best_cart_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EBM** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6419437340153452\n"
     ]
    }
   ],
   "source": [
    "#basic ebm:\n",
    "ebm = ExplainableBoostingClassifier(random_state=42, n_jobs=-1)\n",
    "ebm.fit(X_smote, y_smote)\n",
    "\n",
    "ebm_pred = ebm.predict(X_test)\n",
    "ebm_accuracy = accuracy_score(y_test, ebm_pred)\n",
    "print(\"Accuracy:\", ebm_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'early_stopping_rounds': 10, 'interactions': 2, 'learning_rate': 0.01, 'max_bins': 256, 'max_interaction_bins': 32, 'min_samples_leaf': 10}\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tuning \n",
    "\n",
    "ebm_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_bins': [256, 512],\n",
    "    'max_interaction_bins': [16, 32, 64],\n",
    "    'interactions': [0, 2, 5],\n",
    "    'min_samples_leaf': [1, 10],\n",
    "    'early_stopping_rounds': [10, 50]\n",
    "}\n",
    "\n",
    "ebm_grid_search = GridSearchCV(estimator=ebm, param_grid=ebm_param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "ebm_grid_search.fit(X_smote, y_smote)\n",
    "print(\"Best Hyperparameters:\", ebm_grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Best Hyperparameters: 0.6547314578005116\n"
     ]
    }
   ],
   "source": [
    "#best ebm:\n",
    "best_ebm = ExplainableBoostingClassifier(**ebm_grid_search.best_params_)\n",
    "best_ebm.fit(X_smote, y_smote)\n",
    "best_ebm_pred = best_ebm.predict(X_test)\n",
    "best_ebm_accuracy = accuracy_score(y_test, best_ebm_pred)\n",
    "print(\"Accuracy with Best Hyperparameters:\", best_ebm_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6784112748238309\n"
     ]
    }
   ],
   "source": [
    "best_ebm_train = best_ebm.predict(X_train)\n",
    "print(accuracy_score(y_train, best_ebm_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear SVM** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.659846547314578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lnick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lnick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#basic linear svm:\n",
    "lsvm = LinearSVC(C=1.0, random_state=42)\n",
    "lsvm.fit(X_smote, y_smote)\n",
    "\n",
    "y_pred = lsvm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'C': 1, 'intercept_scaling': 10, 'loss': 'squared_hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lnick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lnick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tuning \n",
    "\n",
    "lsvm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'intercept_scaling': [0.1, 1, 10],\n",
    "    'loss': ['hinge', 'squared_hinge']\n",
    "}\n",
    "\n",
    "\n",
    "lsvm_grid_search = GridSearchCV(estimator=lsvm, param_grid=lsvm_param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "lsvm_grid_search.fit(X_smote, y_smote)\n",
    "print(\"Best Hyperparameters:\", lsvm_grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lnick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Best Hyperparameters: 0.6572890025575447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lnick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#best lsvm:\n",
    "best_lsvm = LinearSVC(**lsvm_grid_search.best_params_)\n",
    "best_lsvm.fit(X_smote, y_smote)\n",
    "best_lsvm_pred = best_lsvm.predict(X_test)\n",
    "best_lsvm_accuracy = accuracy_score(y_test, best_lsvm_pred)\n",
    "print(\"Accuracy with Best Hyperparameters:\", best_lsvm_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6329276105060858\n"
     ]
    }
   ],
   "source": [
    "best_lsvm_train = best_lsvm.predict(X_train)\n",
    "print(accuracy_score(y_train, best_lsvm_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGBoost** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5959079283887468\n"
     ]
    }
   ],
   "source": [
    "#basic xgboost:\n",
    "xgboost = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgboost.fit(X_smote, y_smote)\n",
    "\n",
    "y_pred = xgboost.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "Best Hyperparameters: {'colsample_bytree': 0.5, 'gamma': 10, 'max_depth': 5, 'min_child_weight': 10, 'n_estimators': 250, 'subsample': 0.4}\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tuning \n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 250, 500], \n",
    "    'max_depth': [2, 5, 10],      \n",
    "    'min_child_weight': [1, 5, 10],  \n",
    "    'subsample': [0.4, 0.6, 0.8],   \n",
    "    'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "    'gamma': [0, 1, 10]       \n",
    "}\n",
    "\n",
    "\n",
    "xgb_grid_search = GridSearchCV(estimator=xgboost, param_grid=xgb_param_grid, cv=5, n_jobs=-1, verbose=2, scoring='roc_auc')\n",
    "xgb_grid_search.fit(X_smote, y_smote)\n",
    "print(\"Best Hyperparameters:\", xgb_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Best Hyperparameters: 0.6265984654731458\n"
     ]
    }
   ],
   "source": [
    "#best xgb:\n",
    "best_xgb = xgb.XGBClassifier(**xgb_grid_search.best_params_)\n",
    "best_xgb.fit(X_smote, y_smote)\n",
    "best_xgb_pred = best_xgb.predict(X_test)\n",
    "best_xgb_accuracy = accuracy_score(y_test, best_xgb_pred)\n",
    "print(\"Accuracy with Best Hyperparameters:\", best_xgb_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6457399103139013\n"
     ]
    }
   ],
   "source": [
    "best_xgb_train = best_xgb.predict(X_train)\n",
    "print(accuracy_score(y_train, best_xgb_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Neural Network** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 3ms/step - loss: 0.9828 - accuracy: 0.6164\n",
      "Accuracy: 0.616368293762207\n"
     ]
    }
   ],
   "source": [
    "neural_net = Sequential()\n",
    "neural_net.add(Dense(128, activation='relu', input_shape=(X_smote.shape[1],)))\n",
    "neural_net.add(Dense(64, activation='relu'))\n",
    "neural_net.add(Dense(1, activation='sigmoid')) #for binary features\n",
    "\n",
    "neural_net.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = neural_net.fit(X_smote, y_smote, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "loss, accuracy = neural_net.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam', activation='relu', neurons=128):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation=activation, input_shape=(X_smote.shape[1],)))\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'optimizer': 'sgd', 'activation': 'relu', 'neurons': 128, 'batch_size': 32, 'epoch': 10}\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter grid\n",
    "optimizers = ['adam', 'adagrad', 'sgd']\n",
    "activations = ['relu', 'tanh']\n",
    "neuron_numbers = [128, 256, 512]\n",
    "batch_sizes = [32, 64]\n",
    "epochs = [10, 50]\n",
    "\n",
    "best_nn_accuracy = 0\n",
    "best_nn_params = {}\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    for activation in activations:\n",
    "        for neurons in neuron_numbers:\n",
    "            for sizes in batch_sizes:\n",
    "                for epoch in epochs:\n",
    "                    # Create and train the model\n",
    "                    nn = create_model(optimizer=optimizer, activation=activation, neurons=neurons)\n",
    "                    nn.fit(X_smote, y_smote, epochs=epoch, batch_size=sizes, verbose=0)\n",
    "                    \n",
    "                    # Evaluate the model\n",
    "                    loss, accuracy = nn.evaluate(X_test, y_test, verbose=0)\n",
    "                    \n",
    "                    # Compare and store the best parameters\n",
    "                    if accuracy > best_nn_accuracy:\n",
    "                        best_nn_accuracy = accuracy\n",
    "                        best_nn_params = {'optimizer': optimizer, 'activation': activation, 'neurons': neurons,\n",
    "                                          'batch_size': sizes, 'epoch': epoch}\n",
    "\n",
    "print(f\"Best Parameters: {best_nn_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step\n",
      "Accuracy with Best Hyperparameters: 0.6624040920716112\n"
     ]
    }
   ],
   "source": [
    "#best nn:\n",
    "best_nn = create_model(optimizer=best_nn_params['optimizer'], activation=best_nn_params['activation'], neurons=best_nn_params['neurons'])\n",
    "best_nn.fit(X_smote, y_smote, epochs=epoch, batch_size=sizes, verbose=0)\n",
    "\n",
    "best_nn_pred = best_nn.predict(X_test)\n",
    "best_nn_pred = (best_nn_pred > 0.5).astype(int).ravel()\n",
    "best_nn_accuracy = accuracy_score(y_test, best_nn_pred)\n",
    "print(\"Accuracy with Best Hyperparameters:\", best_nn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step\n",
      "0.6643177450352338\n"
     ]
    }
   ],
   "source": [
    "best_nn_train = best_nn.predict(X_train)\n",
    "best_nn_train = (best_nn_train > 0.5).astype(int).ravel()\n",
    "print(accuracy_score(y_train, best_nn_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **count misclassifications of the training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2719\n"
     ]
    }
   ],
   "source": [
    "misclassifications = []\n",
    "misclass_count=0\n",
    "for i in range(len(y_train)):\n",
    "    \n",
    "    misclassification_count = 0\n",
    "    # Compare predictions with true label for each model\n",
    "    if best_cart_train[i] != y_train[i]:\n",
    "        misclass_count += 1\n",
    "        misclassification_count += 1\n",
    "    if best_ebm_train[i] != y_train[i]:\n",
    "        misclass_count += 1\n",
    "        misclassification_count += 1\n",
    "    if best_lsvm_train[i] != y_train[i]:\n",
    "        misclass_count += 1\n",
    "        misclassification_count += 1\n",
    "    if best_xgb_train[i] != y_train[i]:\n",
    "        misclass_count += 1\n",
    "        misclassification_count += 1\n",
    "    if best_nn_train[i] != y_train[i]:\n",
    "        misclass_count += 1\n",
    "        misclassification_count += 1\n",
    "    misclassification_rate = misclassification_count / 5\n",
    "    # Append the misclassification count for this sample to the list\n",
    "    misclassifications.append(misclassification_rate)\n",
    "\n",
    "\n",
    "# Add the misclassifications as a new column in your dataset or as a separate array\n",
    "# Assuming you have a DataFrame named 'data'\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_train_df['misclassifications'] = misclassifications\n",
    "\n",
    "print(misclass_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **make a new .csv that includes misclassification rate as a training feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X_train_df.to_csv('train_with_misclass.csv', index=False)\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train, columns=['general_two_year'])\n",
    "\n",
    "\n",
    "train_data = pd.concat([X_train_df, y_train_df], axis=1)\n",
    "train_data.to_csv('train_with_misclass.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
